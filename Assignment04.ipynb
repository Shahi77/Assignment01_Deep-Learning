{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7gM0zNhsgH3UZxhRfEOTe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shahi77/DeepLearning_Assignments/blob/main/Assignment04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Implement the dropout function for a single layer. Draw samples from the uniform distribution S[0,1] . Keep those nodes for which the corresponding sample is greater than p probability , dropping the rest. Implement a dropout_layer function that drops out the elements in the tensor input X with probability dropout, rescaling the remainder. Test the dropout layer with few examples"
      ],
      "metadata": {
        "id": "Wo9YdB-kdXYl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nMPwz5Z6At7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31e08c11-299d-401e-fc02-4a3252279bac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropout Probability: 0.0\n",
            "Dropout Probability: 0.3\n",
            "Original Input:\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.],\n",
            "        [7., 8., 9.]])\n",
            "Generated Mask:\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [0., 1., 1.]])\n",
            "Dropout Applied Output:\n",
            "tensor([[ 1.4286,  2.8571,  4.2857],\n",
            "        [ 5.7143,  7.1429,  8.5714],\n",
            "        [ 0.0000, 11.4286, 12.8571]])\n",
            "Percentage of dropped values: 11.11%\n",
            "Mean of input: 5.0000, Mean of output: 6.0317\n",
            "-\n",
            "Dropout Probability: 0.5\n",
            "Original Input:\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.],\n",
            "        [7., 8., 9.]])\n",
            "Generated Mask:\n",
            "tensor([[0., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [0., 1., 1.]])\n",
            "Dropout Applied Output:\n",
            "tensor([[ 0.,  4.,  6.],\n",
            "        [ 8., 10., 12.],\n",
            "        [ 0., 16., 18.]])\n",
            "Percentage of dropped values: 22.22%\n",
            "Mean of input: 5.0000, Mean of output: 8.2222\n",
            "-\n",
            "Dropout Probability: 0.8\n",
            "Original Input:\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.],\n",
            "        [7., 8., 9.]])\n",
            "Generated Mask:\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 1.],\n",
            "        [0., 0., 0.]])\n",
            "Dropout Applied Output:\n",
            "tensor([[ 0.,  0.,  0.],\n",
            "        [ 0.,  0., 30.],\n",
            "        [ 0.,  0.,  0.]])\n",
            "Percentage of dropped values: 88.89%\n",
            "Mean of input: 5.0000, Mean of output: 3.3333\n",
            "-\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def dropout_layer(X: torch.Tensor, dropout: float) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Applies dropout to a given input tensor X.\n",
        "\n",
        "    Parameters:\n",
        "    X (torch.Tensor): Input tensor.\n",
        "    dropout (float): Dropout probability (0 <= dropout < 1). Higher means more units dropped.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: Tensor after applying dropout (scaled appropriately).\n",
        "    \"\"\"\n",
        "    assert 0 <= dropout < 1, \"Dropout probability must be in the range [0, 1).\"\n",
        "\n",
        "    if dropout == 0:\n",
        "        return X  # No dropout applied\n",
        "\n",
        "    # Generate a mask with the same shape as X, using uniform distribution\n",
        "    mask = (torch.rand_like(X) > dropout).float()\n",
        "\n",
        "    # Scale the remaining elements\n",
        "    output = (X * mask) / (1.0 - dropout)\n",
        "\n",
        "    # Debugging info\n",
        "    print(\"Original Input:\")\n",
        "    print(X)\n",
        "    print(\"Generated Mask:\")\n",
        "    print(mask)\n",
        "    print(\"Dropout Applied Output:\")\n",
        "    print(output)\n",
        "    print(f\"Percentage of dropped values: {100 * (1 - mask.mean().item()):.2f}%\")\n",
        "    print(f\"Mean of input: {X.mean().item():.4f}, Mean of output: {output.mean().item():.4f}\")\n",
        "    print(\"-\")\n",
        "\n",
        "    return output\n",
        "\n",
        "# Testing the dropout layer with a few examples\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(42)  # For reproducibility\n",
        "\n",
        "    # Example input tensor\n",
        "    X = torch.tensor([[1.0, 2.0, 3.0],\n",
        "                      [4.0, 5.0, 6.0],\n",
        "                      [7.0, 8.0, 9.0]])\n",
        "\n",
        "    dropout_rates = [0.0, 0.3, 0.5, 0.8]\n",
        "\n",
        "    for p in dropout_rates:\n",
        "        print(f\"Dropout Probability: {p}\")\n",
        "        dropout_layer(X, p)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement Dropout layer in neural network model after every fully connected layer"
      ],
      "metadata": {
        "id": "G5bzsnkdfo4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, dropout_prob):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Testing the neural network with dropout\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(42)  # For reproducibility\n",
        "\n",
        "    # Example input tensor\n",
        "    X = torch.randn(5, 10)  # Batch of 5 samples, 10 features each\n",
        "\n",
        "    model = NeuralNetwork(input_size=10, hidden_size=20, output_size=5, dropout_prob=0.3)\n",
        "    model.eval()  # Set model to evaluation mode (no dropout applied)\n",
        "    print(\"Output without dropout (evaluation mode):\")\n",
        "    print(model(X))\n",
        "\n",
        "    model.train()  # Set model to training mode (dropout applied)\n",
        "    print(\"Output with dropout (training mode):\")\n",
        "    print(model(X))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvagTsl5fY8m",
        "outputId": "b1932eb2-1634-4ae6-df15-8bb987ddbc8a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output without dropout (evaluation mode):\n",
            "tensor([[ 0.0713, -0.0538,  0.0075, -0.3819, -0.2163],\n",
            "        [-0.0048,  0.1738,  0.2743, -0.2746, -0.1560],\n",
            "        [ 0.0465,  0.0078,  0.2278, -0.1960, -0.0924],\n",
            "        [-0.0017,  0.0759,  0.2682, -0.2008, -0.1750],\n",
            "        [ 0.0606,  0.0299,  0.2935, -0.2621, -0.1218]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Output with dropout (training mode):\n",
            "tensor([[ 0.1122, -0.1865,  0.0692, -0.4563, -0.2607],\n",
            "        [-0.0957,  0.4482,  0.2956, -0.2055, -0.1704],\n",
            "        [ 0.0796,  0.1718,  0.2367, -0.2209, -0.1437],\n",
            "        [-0.0208,  0.2398,  0.3445, -0.2174, -0.1471],\n",
            "        [ 0.0470,  0.1374,  0.4373, -0.4032, -0.0424]],\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Visualization using wandb library for various experimental setups."
      ],
      "metadata": {
        "id": "GEYYKx8of0Cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "\n",
        "# Initialize Weights Function\n",
        "def initialize_weights(m, strategy=\"random\"):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        if strategy == \"random\":\n",
        "            nn.init.normal_(m.weight, mean=0, std=0.01)\n",
        "        elif strategy == \"pretrained\":\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "        elif strategy == \"threshold\":\n",
        "            nn.init.uniform_(m.weight, a=-0.5, b=0.5)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "# Define Standard Neural Network\n",
        "class StandardNeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, activation, dropout_prob=0.0):\n",
        "        super(StandardNeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.activation = activation\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define Dropout Neural Network\n",
        "class DropoutNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, activation, dropout_prob):\n",
        "        super(DropoutNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"dropout-experiments\")\n",
        "\n",
        "# Define Experiments\n",
        "configs = [\n",
        "    {\"model\": StandardNeuralNet, \"activation\": F.logsigmoid, \"layers\": 2, \"units\": 100},\n",
        "    {\"model\": StandardNeuralNet, \"activation\": F.logsigmoid, \"layers\": 2, \"units\": 800},\n",
        "    {\"model\": DropoutNN, \"activation\": F.logsigmoid, \"layers\": 3, \"units\": 1024},\n",
        "    {\"model\": DropoutNN, \"activation\": F.relu, \"layers\": 3, \"units\": 1024}\n",
        "]\n",
        "\n",
        "# Run Experiments\n",
        "for config in configs:\n",
        "    model = config[\"model\"](input_size=784, hidden_size=config[\"units\"], output_size=10, activation=config[\"activation\"], dropout_prob=0.3)\n",
        "    model.apply(lambda m: initialize_weights(m, strategy=\"random\"))\n",
        "    wandb.log({\"model\": str(model), \"units\": config[\"units\"], \"layers\": config[\"layers\"]})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "P0Xc8uZCfzsT",
        "outputId": "b595bdb2-595d-43b3-d4cd-aae43750f9ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshahi77\u001b[0m (\u001b[33mshahi77-national-institute-of-technology-hamirpur\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250214_081525-521jzkf2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shahi77-national-institute-of-technology-hamirpur/dropout-experiments/runs/521jzkf2' target=\"_blank\">alluring-candy-1</a></strong> to <a href='https://wandb.ai/shahi77-national-institute-of-technology-hamirpur/dropout-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/shahi77-national-institute-of-technology-hamirpur/dropout-experiments' target=\"_blank\">https://wandb.ai/shahi77-national-institute-of-technology-hamirpur/dropout-experiments</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/shahi77-national-institute-of-technology-hamirpur/dropout-experiments/runs/521jzkf2' target=\"_blank\">https://wandb.ai/shahi77-national-institute-of-technology-hamirpur/dropout-experiments/runs/521jzkf2</a>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}